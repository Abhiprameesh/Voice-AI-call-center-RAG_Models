{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPWLye+6lkXx0HpPx0s+Q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhiprameesh/Voice-AI-call-center-RAG_Models/blob/main/RAG_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Installing the Required Dependencies\n",
        "!pip install faiss-cpu sentence-transformers transformers pandas --quiet\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import os\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "# 2. Config (Choose Backend)\n",
        "USE_OPENAI = False  # Change to True if you want to use OpenAI API\n",
        "# 3. Load Datasetcontent/\n",
        "df = pd.read_csv(\"RAGCHAT1.csv\")\n",
        "print(\"Sample rows:\")\n",
        "print(df.head())\n",
        "# Expected Columns: [\"Intent (Category)\", \"User Question Variation\", \"Official Bot Response\", \"Source URL\", \"Notes\"]\n",
        "# 4. Embeddings + FAISS Index\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = embedder.encode(df[\"User Question Variation\"].tolist(), normalize_embeddings=True)\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "index.add(embeddings)\n",
        "print(f\"FAISS index built with {index.ntotal} entries.\")\n",
        "# 5. Save / Load Pickle\n",
        "def save_index(filename=\"rag_index.pkl\"):\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump((df, index), f)\n",
        "    print(f\" Saved index to {filename}\")\n",
        "\n",
        "def load_index(filename=\"rag_index.pkl\"):\n",
        "    global df, index\n",
        "    with open(filename, \"rb\") as f:\n",
        "        df, index = pickle.load(f)\n",
        "    print(f\"Loaded index from {filename}\")\n",
        "# 6. Retrieval Function\n",
        "def retrieve_answer(query, top_k=2):\n",
        "    query_vec = embedder.encode([query], normalize_embeddings=True)\n",
        "    scores, indices = index.search(query_vec, top_k)\n",
        "\n",
        "    retrieved_data = []\n",
        "    for idx, score in zip(indices[0], scores[0]):\n",
        "        if idx != -1:\n",
        "            row = df.iloc[idx]\n",
        "            retrieved_data.append(f\"- {row['Official Bot Response']} (Source: {row['Source URL']})\")\n",
        "    return \"\\n\".join(retrieved_data)\n",
        "# 7. Choose LLM Backend\n",
        "if USE_OPENAI:\n",
        "    from openai import OpenAI\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # replace with your key\n",
        "    client = OpenAI()\n",
        "\n",
        "    def respectful_bot(query):\n",
        "        context = retrieve_answer(query, top_k=2)\n",
        "        if not context.strip():\n",
        "            return \"üôè I don‚Äôt know the answer. Please check the BBMP official website.\"\n",
        "\n",
        "        template = f\"\"\"\n",
        "You are a respectful municipal chatbot.\n",
        "Answer the user question ONLY using the context below.\n",
        "If the answer is not in the context, reply exactly: \"I don‚Äôt know the answer.\"\n",
        "\n",
        "User Question: {query}\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a polite municipal assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": template}\n",
        "            ],\n",
        "            temperature=0\n",
        "        )\n",
        "        return response.choices[0].message[\"content\"]\n",
        "\n",
        "else:\n",
        "    rag_llm = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=-1)\n",
        "\n",
        "    def respectful_bot(query):\n",
        "        context = retrieve_answer(query, top_k=2)\n",
        "        if not context.strip():\n",
        "            return \"üôè I don‚Äôt know the answer. Please check the BBMP official website.\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a respectful municipal chatbot.\n",
        "Answer the user question ONLY using the context below.\n",
        "If the answer is not in the context, reply exactly: \"I don‚Äôt know the answer.\"\n",
        "\n",
        "User Question: {query}\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "        result = rag_llm(prompt, max_length=256, temperature=0)[0]['generated_text']\n",
        "        return result\n",
        "# 8. Test Queries\n",
        "test_queries = [\n",
        "    \"How do I dispose hazardous waste like batteries, e-waste?\",\n",
        "    \"How can I appeal against property tax penalties?\",\n",
        "    \"What is the fastest way to complaint the BBMP about road damage?\",\n",
        "    \"Who is the Prime Minister of India?\"  # should say \"I don‚Äôt know the answer.\"\n",
        "]\n",
        "for q in test_queries:\n",
        "    print(\"User:\", q)\n",
        "    print(\"Bot:\", respectful_bot(q))\n",
        "    print(\"-\" * 60)\n",
        "# 9. Save Index (for later use)\n",
        "save_index(\"RAG_index_1.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yVOSEHQM891",
        "outputId": "27f808e0-6511-4f35-e3b2-fa8d07c34613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample rows:\n",
            "      Intent (Category)                            User Question Variation  \\\n",
            "0  ask_garbage_schedule         What time is garbage collected in my area?   \n",
            "1  ask_garbage_schedule      When does garbage pickup happen in my street?   \n",
            "2   ask_collection_days        Which days is garbage collected in my area?   \n",
            "3     ask_missed_pickup   My garbage was not picked up today‚Äîwhat do I do?   \n",
            "4        ask_bulk_waste  How can I dispose of old furniture or bulky wa...   \n",
            "\n",
            "                               Official Bot Response  \\\n",
            "0  As per the latest BBMP update effective from A...   \n",
            "1  BBMP collects waste door-to-door beginning at ...   \n",
            "2  BBMP arranges daily wet waste collection for m...   \n",
            "3  We regret the inconvenience. Missed pickups mu...   \n",
            "4  Bulk waste such as furniture, mattresses, and ...   \n",
            "\n",
            "                                          Source URL  \\\n",
            "0  https://www.hindustantimes.com/cities/bengalur...   \n",
            "1  https://www.goodreturns.in/news/bengaluru-wast...   \n",
            "2  https://site.bbmp.gov.in/documents/Schedule%20...   \n",
            "3  https://www.thehindu.com/news/national/karnata...   \n",
            "4  https://www.goodreturns.in/news/bengaluru-wast...   \n",
            "\n",
            "                                               Notes  \n",
            "0     Schedule shifted earlier from August 25, 2025.  \n",
            "1  Residents should keep bins outside before pick...  \n",
            "2  Schedules differ by ward, so ask for ward numb...  \n",
            "3  Missed pickup complaints are given high priori...  \n",
            "4  Bulk pickups are by pre-arrangement and may in...  \n",
            "FAISS index built with 316 entries.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: How do I dispose hazardous waste like batteries, e-waste?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: at designated collection centers or through authorized channels to prevent environmental contamination\n",
            "------------------------------------------------------------\n",
            "User: How can I appeal against property tax penalties?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Complete the appeal form on the BBMP portal with proper documentation explaining the grounds. Appeals are reviewed by officers; decisions communicated online. (Source: https://bbmptax.karnataka.gov.in/forms/Complaintrequest.aspx)\n",
            "------------------------------------------------------------\n",
            "User: What is the fastest way to complaint the BBMP about road damage?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: The quickest way is to register your complaint on the BBMP‚Äôs dedicated ‚ÄúFix Pothole‚Äù Android app or call the 1533 helpline. WhatsApp messaging to local ward numbers with location and photos also speeds up notification and response. (Source: https://play.google.com/store/apps/details?id=com.indigo.bbmp.fixpothole)\n",
            "------------------------------------------------------------\n",
            "User: Who is the Prime Minister of India?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: I don‚Äôt know the answer.\n",
            "------------------------------------------------------------\n",
            " Saved index to RAG_index_1.pkl\n"
          ]
        }
      ]
    }
  ]
}